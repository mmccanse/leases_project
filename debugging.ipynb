{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from qdrant_client import QdrantClient, models\n",
    "import qdrant_client\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "import logging\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name for LLM\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "# Store API key as a variable\n",
    "openai_api_key = st.secrets[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store():\n",
    "    #Create a client to connect to Qdrant server\n",
    "    client = qdrant_client.QdrantClient(\n",
    "        st.secrets[\"QDRANT_HOST\"],\n",
    "        api_key=st.secrets[\"QDRANT_API_KEY\"]\n",
    "        )\n",
    "    \n",
    "    #initialize embeddings for vector store\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        api_key=openai_api_key,\n",
    "        model=\"text-embedding-3-large\"\n",
    "    )\n",
    "    \n",
    "    # create vector_store with Qdrant and embeddings\n",
    "    vector_store = Qdrant(\n",
    "        client = client,\n",
    "        collection_name = st.secrets[\"QDRANT_COLLECTION_NAME\"],\n",
    "        embeddings = embeddings,\n",
    "    )\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='C:\\\\Users\\\\mered\\\\anaconda3\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='C:\\\\Users\\\\mered\\\\anaconda3\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='C:\\\\Users\\\\mered\\\\anaconda3\\\\Library\\\\ssl\\\\cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "vector_store = get_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to setup prompt template\n",
    "def setup_prompt_template():\n",
    "    prefix=\"\"\"You are a leases chatbot. You answer questions relating to ASC 842 under US GAAP. You respond to the queries as shown in \n",
    "    the examples. The responses do not have to be brief. Giving a thorough response with citations from the source documents is more\n",
    "    important than brevity. Each response will be followed by reference from multiple sources with section numbers or page numbers (which\n",
    "    is in the meta data) from the source documents. The responses will be provided only from the provided PDF source documents.  \n",
    "    The responses will be clear and helpful and will use language that is easy to understand. Responses will include examples and \n",
    "    potential scenarios.  If the answer is not avaiable in the PDF source documents, the response will be \"I do not have information related \n",
    "    to that specific scenario, please seek guidance from a qualified expert.\" If the question is not on the topic of leases, respond by \n",
    "    saying, \"This is outside the scope of what I can help you with. Let's get back to lease accounting.\" \n",
    "    \n",
    "    Context from source documents:\n",
    "    {context}\n",
    "\n",
    "    Proceed with answering the following questions based on the above context:\"\"\"\n",
    "     \n",
    "     # Define examples to instruct app how to respond\n",
    "    examples = [\n",
    "        {\n",
    "            \"query\": \"How do I determine the different lease components?\",\n",
    "            \"answer\": \"\"\"Lease components are elements of the arrangement that provide the customer with the right to use an \n",
    "            identified asset. An entity should consider the right to use an underlying asset to be a separate lease component \n",
    "            if the following 2 conditions are met: 1. Lessee can benefit from the ROU asset either on its own or together with \n",
    "            other readily available resources and 2. The ROU is neither highly dependent on; nor highly interrelated with the \n",
    "            other ROU(s) in the contract. You can find additional information in the following reference documents. References: \n",
    "            KPMG-leaseshandbook section 4.1, PWC-leasesguide0124, section 2.4, EY-financial-reporting-developments-lease-\n",
    "            accounting, section 1.4\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"I am a lessor, how do I account for lease modifications?\",\n",
    "            \"answer\": \"\"\"Several questions must be answered to determine how to appropriately account for lease modifications for \n",
    "            a lessor. Is the modified contract a lease, or does it contain a lease? If yes, does the modification result in a \n",
    "            separate contract? If yes, account for two separate contracts: the unmodified original contract, and a separate \n",
    "            contract accounted for in the same manner as any other new lease. If the modification does not result in a separate \n",
    "            contract, remeasure and reallocate the remaining consideration in the contract, reassess the lease classification at \n",
    "            the modification effective date, and account for any initial direct costs, lease incentives, and other paymetns made to \n",
    "            or by the lessor. Whether or not the lease classification changes, and how it changes drives the appropriate accounting. \n",
    "            You can find additional information in the following reference documents. References: EY - Financial Reporting Developments: \n",
    "            lease accounting, section 5.6, PWC - Leases Guide, section 5.6, KPMG - Leases Handbook, section 7.6\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    #Define format for examples:\n",
    "    example_format = \"\"\"Human: {query}\\nAI: {answer}\"\"\"\n",
    "    \n",
    "    example_template = PromptTemplate(input_variables=[\"query\", \"answer\"],\n",
    "                                      template=example_format)    \n",
    "    #Define suffix for query\n",
    "    suffix=\"\\n\\nHuman: {query}\\nAI: \"\n",
    "    \n",
    "    #Construct FewShotPromptTemplate\n",
    "    prompt_template = FewShotPromptTemplate(\n",
    "                                            examples=examples,\n",
    "                                            example_prompt=example_template,\n",
    "                                            input_variables=[\"query\"],\n",
    "                                            prefix=prefix,\n",
    "                                            suffix=suffix,\n",
    "                                            example_separator=\"\\n\")\n",
    "\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'query'] examples=[{'query': 'How do I determine the different lease components?', 'answer': 'Lease components are elements of the arrangement that provide the customer with the right to use an \\n            identified asset. An entity should consider the right to use an underlying asset to be a separate lease component \\n            if the following 2 conditions are met: 1. Lessee can benefit from the ROU asset either on its own or together with \\n            other readily available resources and 2. The ROU is neither highly dependent on; nor highly interrelated with the \\n            other ROU(s) in the contract. You can find additional information in the following reference documents. References: \\n            KPMG-leaseshandbook section 4.1, PWC-leasesguide0124, section 2.4, EY-financial-reporting-developments-lease-\\n            accounting, section 1.4'}, {'query': 'I am a lessor, how do I account for lease modifications?', 'answer': 'Several questions must be answered to determine how to appropriately account for lease modifications for \\n            a lessor. Is the modified contract a lease, or does it contain a lease? If yes, does the modification result in a \\n            separate contract? If yes, account for two separate contracts: the unmodified original contract, and a separate \\n            contract accounted for in the same manner as any other new lease. If the modification does not result in a separate \\n            contract, remeasure and reallocate the remaining consideration in the contract, reassess the lease classification at \\n            the modification effective date, and account for any initial direct costs, lease incentives, and other paymetns made to \\n            or by the lessor. Whether or not the lease classification changes, and how it changes drives the appropriate accounting. \\n            You can find additional information in the following reference documents. References: EY - Financial Reporting Developments: \\n            lease accounting, section 5.6, PWC - Leases Guide, section 5.6, KPMG - Leases Handbook, section 7.6'}] example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='Human: {query}\\nAI: {answer}') suffix='\\n\\nHuman: {query}\\nAI: ' example_separator='\\n' prefix='You are a leases chatbot. You answer questions relating to ASC 842 under US GAAP. You respond to the queries as shown in \\n    the examples. The responses do not have to be brief. Giving a thorough response with citations from the source documents is more\\n    important than brevity. Each response will be followed by reference from multiple sources with section numbers or page numbers (which\\n    is in the meta data) from the source documents. The responses will be provided only from the provided PDF source documents.  \\n    The responses will be clear and helpful and will use language that is easy to understand. Responses will include examples and \\n    potential scenarios.  If the answer is not avaiable in the PDF source documents, the response will be \"I do not have information related \\n    to that specific scenario, please seek guidance from a qualified expert.\" If the question is not on the topic of leases, respond by \\n    saying, \"This is outside the scope of what I can help you with. Let\\'s get back to lease accounting.\" \\n    \\n    Context from source documents:\\n    {context}\\n\\n    Proceed with answering the following questions based on the above context:'\n"
     ]
    }
   ],
   "source": [
    "#initialize prompt template\n",
    "prompt_template = setup_prompt_template()\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='C:\\\\Users\\\\mered\\\\anaconda3\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='C:\\\\Users\\\\mered\\\\anaconda3\\\\Library\\\\ssl\\\\cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the disclosure requirements under ASC 842?\"\n",
    "llm = ChatOpenAI(api_key=openai_api_key, model=OPENAI_MODEL)\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Prompt must accept context as an input variable. Received prompt with input variables: ['query']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m combine_docs_chain \u001b[38;5;241m=\u001b[39m create_stuff_documents_chain(\n\u001b[0;32m      2\u001b[0m     llm, prompt_template\n\u001b[0;32m      3\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mered\\anaconda3\\Lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:74\u001b[0m, in \u001b[0;36mcreate_stuff_documents_chain\u001b[1;34m(llm, prompt, output_parser, document_prompt, document_separator)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_stuff_documents_chain\u001b[39m(\n\u001b[0;32m     23\u001b[0m     llm: LanguageModelLike,\n\u001b[0;32m     24\u001b[0m     prompt: BasePromptTemplate,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     document_separator: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DOCUMENT_SEPARATOR,\n\u001b[0;32m     29\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Any]:\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chain for passing a list of Documents to a model.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m            chain.invoke({\"context\": docs})\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m     _validate_prompt(prompt)\n\u001b[0;32m     75\u001b[0m     _document_prompt \u001b[38;5;241m=\u001b[39m document_prompt \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_DOCUMENT_PROMPT\n\u001b[0;32m     76\u001b[0m     _output_parser \u001b[38;5;241m=\u001b[39m output_parser \u001b[38;5;129;01mor\u001b[39;00m StrOutputParser()\n",
      "File \u001b[1;32mc:\\Users\\mered\\anaconda3\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:26\u001b[0m, in \u001b[0;36m_validate_prompt\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_prompt\u001b[39m(prompt: BasePromptTemplate) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DOCUMENTS_KEY \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m prompt\u001b[38;5;241m.\u001b[39minput_variables:\n\u001b[1;32m---> 26\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     27\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt must accept \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDOCUMENTS_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as an input variable. Received prompt \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith input variables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;241m.\u001b[39minput_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Prompt must accept context as an input variable. Received prompt with input variables: ['query']"
     ]
    }
   ],
   "source": [
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='C:\\\\Users\\\\mered\\\\anaconda3\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='C:\\\\Users\\\\mered\\\\anaconda3\\\\Library\\\\ssl\\\\cacert.pem'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Prompt must accept context as an input variable. Received prompt with input variables: ['query']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(api_key\u001b[38;5;241m=\u001b[39mopenai_api_key, model\u001b[38;5;241m=\u001b[39mOPENAI_MODEL)\n\u001b[0;32m      3\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39mas_retriever()\n\u001b[1;32m----> 4\u001b[0m combine_docs_chain \u001b[38;5;241m=\u001b[39m create_stuff_documents_chain(\n\u001b[0;32m      5\u001b[0m     llm, prompt_template\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m retrieval_chain \u001b[38;5;241m=\u001b[39m create_retrieval_chain(retriever, combine_docs_chain)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mered\\anaconda3\\Lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:74\u001b[0m, in \u001b[0;36mcreate_stuff_documents_chain\u001b[1;34m(llm, prompt, output_parser, document_prompt, document_separator)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_stuff_documents_chain\u001b[39m(\n\u001b[0;32m     23\u001b[0m     llm: LanguageModelLike,\n\u001b[0;32m     24\u001b[0m     prompt: BasePromptTemplate,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     document_separator: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DOCUMENT_SEPARATOR,\n\u001b[0;32m     29\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Any]:\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chain for passing a list of Documents to a model.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m            chain.invoke({\"context\": docs})\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m     _validate_prompt(prompt)\n\u001b[0;32m     75\u001b[0m     _document_prompt \u001b[38;5;241m=\u001b[39m document_prompt \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_DOCUMENT_PROMPT\n\u001b[0;32m     76\u001b[0m     _output_parser \u001b[38;5;241m=\u001b[39m output_parser \u001b[38;5;129;01mor\u001b[39;00m StrOutputParser()\n",
      "File \u001b[1;32mc:\\Users\\mered\\anaconda3\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:26\u001b[0m, in \u001b[0;36m_validate_prompt\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_prompt\u001b[39m(prompt: BasePromptTemplate) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DOCUMENTS_KEY \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m prompt\u001b[38;5;241m.\u001b[39minput_variables:\n\u001b[1;32m---> 26\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     27\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt must accept \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDOCUMENTS_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as an input variable. Received prompt \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith input variables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;241m.\u001b[39minput_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Prompt must accept context as an input variable. Received prompt with input variables: ['query']"
     ]
    }
   ],
   "source": [
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "try:\n",
    "    response = retrieval_chain.invoke({\"input\": question})\n",
    "    answer = response.get(\"answer\", \"no answer generated\")\n",
    "except Exception as e:\n",
    "    answer = f\"An error occurred: {str(e)}\"\n",
    "    \n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #initialize create retrieval chain (experiment)\n",
    "# def initialize_createrc(vector_store):\n",
    "#     llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0, api_key=openai_api_key)\n",
    "#     retriever=vector_store.as_retriever()\n",
    "#     contextualize_q_system_prompt = (\"\"\"\"Given a chat history and the latest user question \"\n",
    "#     \"which might reference context in the chat history, \"\n",
    "#     \"formulate a standalone question which can be understood \"\n",
    "#     \"without the chat history. Do NOT answer the question, just \"\n",
    "#     \"reformulate it if needed and otherwise return it as is.\"\"\")\n",
    "    \n",
    "#     contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "#         [\n",
    "#             (\"system\", contextualize_q_system_prompt),\n",
    "#             MessagesPlaceholder(\"chat_history\"),\n",
    "#             (\"human\", \"{input}\"),\n",
    "#         ]\n",
    "#     )\n",
    "#     history_aware_retriever = create_history_aware_retriever(\n",
    "#         llm, retriever, contextualize_q_prompt\n",
    "#     )\n",
    "    \n",
    "#     # setup for answering the question\n",
    "#     qa_system_prompt = (\n",
    "#         \"\"\"You are a leases chatbot. You answer questions relating to ASC 842 under US GAAP. You respond to the queries as shown in \n",
    "#         the examples. Each response will be followed by reference from multiple sources with section numbers from the source documents. \n",
    "#         The responses will be provided only from the provided PDF source documents.  The responses will be clear and helpful and will use \n",
    "#         language that is easy to understand. Responses will include examples and potential scenarios.  If the answer is not avaiable in\n",
    "#         the PDF source documents, the response will be \"I do not have information related to that specific scenario, please seek guidance\n",
    "#         from a qualified expert.\" \"\"\"\n",
    "#     )\n",
    "#     qa_prompt = ChatPromptTemplate.from_messages(\n",
    "#         [\n",
    "#             (\"system\", qa_system_prompt),\n",
    "#             MessagesPlaceholder(\"retrieved_documents\"),\n",
    "#             (\"human\", \"{context}\"),\n",
    "#         ]\n",
    "#     )\n",
    "    \n",
    "#     # Combine the documents and use the LLM to generate an answer\n",
    "#     stuff_documents_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    \n",
    "#     # Create the complete retrieval chain\n",
    "#     crc = create_retrieval_chain(history_aware_retriever, stuff_documents_chain)\n",
    "\n",
    "#     return crc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #initialize conversational retrieval chain\n",
    "# def initialize_crc(vector_store):\n",
    "#     llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0, api_key=openai_api_key)\n",
    "#     retriever=vector_store.as_retriever()\n",
    "#     crc = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "#     # st.session_state['crc'] = crc\n",
    "#     # st.success('Source documents loaded!')\n",
    "#     return crc "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
